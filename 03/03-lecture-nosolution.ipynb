{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/1.jpg\" width=500>\n",
    "\n",
    "# Напомню Backprop\n",
    "\n",
    "<img src=\"./imgs/2.png\" width=500>\n",
    "\n",
    "Для вычисления производную $\\large \\frac{\\partial L}{\\partial x}$ нам нужно:\n",
    "1. Получить с предыдущего шага производную $\\large \\frac{\\partial L}{\\partial z}$\n",
    "2. Вычислить $\\large \\frac{\\partial z}{\\partial x}$\n",
    "3. Высислить $\\large \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial x}$\n",
    "\n",
    "Далее обновляем веса по алгоритму SGD:\n",
    "$$\n",
    "\\large\n",
    "x = x - \\lambda \\frac{\\partial L}{\\partial x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разбор решения прошлого семинара"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minibatch SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классический SGD:\n",
    "$$\n",
    "\\large\n",
    "x = x - \\lambda \\frac{\\partial L}{\\partial x}\n",
    "$$\n",
    "\n",
    "Mini-batch SGD\n",
    "$$\n",
    "\\large\n",
    "x = x - \\lambda \\frac{1}{B} \\sum_{i=1}^{B}\\frac{\\partial L_i}{\\partial x}\n",
    "$$\n",
    "\n",
    "Мотивация:\n",
    "- Векторизация и утилизация GPU\n",
    "- Ускорение обучения\n",
    "<img src=\"./imgs/3.gif\" width=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Продвинутые методы оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD + Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "\\begin{align}\n",
    "m &= \\alpha m+ (1 - \\alpha) \\nabla_{\\theta} L \\\\\n",
    "\\theta &= \\theta - \\lambda m\n",
    "\\end{align}\n",
    "$$\n",
    "<img src=\"./imgs/4.gif\" width=500>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp (Root Mean Square Propagation)\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\begin{align}\n",
    "v &= \\beta v +  (1-\\beta)(\\nabla_{\\theta} L)^2 \\\\\n",
    "\\theta &= \\theta - \\lambda \\frac{1}{\\sqrt{v}} \\nabla_{\\theta} L\n",
    "\\end{align}\n",
    "$$\n",
    "<img src=\"./imgs/5.gif\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam (Adaptive Moment Estimation) \n",
    "\n",
    "Adam = SGD + Momentum + RMSProp\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\begin{align}\n",
    "m &= \\alpha m+ (1 - \\alpha) \\nabla_{\\theta} L \\\\\n",
    "v &= \\beta v +  (1-\\beta)(\\nabla_{\\theta} L)^2 \\\\\n",
    "\\theta &= \\theta - \\lambda \\frac{m}{\\sqrt{v}} \\nabla_{\\theta} L\n",
    "\\end{align}\n",
    "$$\n",
    "<img src=\"./imgs/6.gif\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Откуда пошли сверточные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1958 - Hubel and Wiesel\n",
    "\n",
    "<img src=\"./imgs/7.jpg\" width=500>\n",
    "\n",
    "<img src=\"./imgs/9.png\" width=500>\n",
    "\n",
    "- **Simple cells**: точки или линии с определенной ориентацией \n",
    "- **Complex cells**: точки или линии с определенной ориентацией двигающиеся в определенном направлении\n",
    "- **Hypercomplex cells**: точки или линии с определенной ориентацией и определенной длинны двигающиеся в определенном направлении "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1980 - Neocognitron (Fukushima)\n",
    "\n",
    "<img src=\"./imgs/10.jpg\" width=400>\n",
    "<img src=\"./imgs/11.gif\" width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1998 LeNet-5 (LeCun et.al.)\n",
    "\n",
    "<img src=\"./imgs/12.png\" width=700>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2012 AlexNet\n",
    "\n",
    "<img src=\"./imgs/14.png\">\n",
    "**ImageNet**\n",
    "- 1000 классов\n",
    "- 1.2М обучение, 100К тест\n",
    "\n",
    "**AlexNet**:\n",
    "<img src=\"./imgs/13.png\">\n",
    "\n",
    "**AlexNet + ImageNet**:\n",
    "<img src=\"./imgs/15.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Взрывной рост GPU\n",
    "<img src=\"./imgs/16.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основные блоки нейронных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/17.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Autograd\n",
    "\n",
    "<img src=\"./imgs/24.png\">\n",
    "\n",
    "- data: $z = f(x)$\n",
    "- creator: $f$\n",
    "- grad: $\\large \\frac{\\partial z}{\\partial x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизация произвольной функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 25697578.0\n",
      "1 161241072.0\n",
      "2 1401235840.0\n",
      "3 1038408896.0\n",
      "4 48567564.0\n",
      "5 65184956.0\n",
      "6 15982612.0\n",
      "7 3403201.5\n",
      "8 1283429.625\n",
      "9 767832.5625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return a\n",
    "        Tensor containing the output. You can cache arbitrary Tensors for use in the\n",
    "        backward pass using the save_for_backward method.\n",
    "        \"\"\"\n",
    "        self.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = self.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs, and wrap them in Variables.\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "learning_rate = 3e-6\n",
    "for t in range(10):\n",
    "    # Construct an instance of our MyReLU class to use in our network\n",
    "    relu = MyReLU()\n",
    "\n",
    "    # Forward pass: compute predicted y using operations on Variables; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.data[0])\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "\n",
    "    # Manually zero the gradients after updating weights\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 697.7476806640625\n",
      "1 645.951416015625\n",
      "2 601.0177612304688\n",
      "3 561.218505859375\n",
      "4 525.8467407226562\n",
      "5 493.89208984375\n",
      "6 464.7021484375\n",
      "7 438.2071533203125\n",
      "8 413.7890625\n",
      "9 390.90631103515625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables.\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Variables for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(10):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Variable of input data to the Module and it produces\n",
    "    # a Variable of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Variables containing the predicted and true\n",
    "    # values of y, and the loss function returns a Variable containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.data[0])\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Variables with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Variable, so\n",
    "    # we can access its data and gradients like we did before.\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 754.0963134765625\n",
      "1 735.7557983398438\n",
      "2 717.9279174804688\n",
      "3 700.6038818359375\n",
      "4 683.7778930664062\n",
      "5 667.4232788085938\n",
      "6 651.50830078125\n",
      "7 636.0040283203125\n",
      "8 620.8887329101562\n",
      "9 606.1624145507812\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables.\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Variables it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(10):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.data[0])\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable weights\n",
    "    # of the model)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сверточный слой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/18.jpg\">\n",
    "\n",
    "<img src=\"./imgs/19.png\">\n",
    "Типичные размеры фильтров: 1х1, 3х3, 5х5,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Картинка = тензор размера (ширина, высота, число каналов)\n",
    "<img src=\"./imgs/20.png\" width=300>\n",
    "\n",
    "1 слой состоит из нескольких филтров (filter bank)\n",
    "<img src=\"./imgs/21.png\" width=400>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stride\n",
    "<img src=\"./imgs/22.gif\" width=300>\n",
    "\n",
    "Padding\n",
    "<img src=\"./imgs/23.gif\" width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Свертка в PyTorch\n",
    "\n",
    "```python\n",
    "class torch.nn.Conv2d(\n",
    "    in_channels,  # Число каналов на входе\n",
    "    out_channels, # Число каналов на выходе\n",
    "    kernel_size,  # Размер ядра свертки\n",
    "    stride=1,     # Шаг свертки\n",
    "    padding=0,    # Сколько прибавлять по краям\n",
    "    dilation=1, groups=1, bias=True) # Параметры не интересные в нашем курсе\n",
    "```\n",
    "\n",
    "- Вход: $(N, C_{in}, H_{in}, W_{in})$\n",
    "- Выход: $(N, C_{out}, H_{out}, W_{out})$\n",
    "- Есть варинаты 1d (для звука, например) и 3d (для объемных томограмм например)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transposed convolution\n",
    "\n",
    "<img src=\"./imgs/25.gif\" width=400>\n",
    "<img src=\"./imgs/28.png\">\n",
    "\n",
    "\n",
    "```python\n",
    "class torch.nn.ConvTranspose2d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling\n",
    "\n",
    "<img src=\"./imgs/26.jpg\" width=400>\n",
    "<img src=\"./imgs/27.jpg\" width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Активация\n",
    "```python\n",
    "class torch.nn.Sigmoid\n",
    "```\n",
    "Чем плох сигмойд?\n",
    "<img src=\"./imgs/29.png\" >\n",
    "<img src=\"./imgs/30.png\" >\n",
    "\n",
    "```python\n",
    "class torch.nn.ReLU(inplace=False)\n",
    "```\n",
    "<img src=\"./imgs/31.png\" >\n",
    "\n",
    "\n",
    "```python\n",
    "class torch.nn.Softmax\n",
    "class torch.nn.Softmax2d\n",
    "```\n",
    "$$\\large {Softmax}_j(x) = \\frac{exp(x_j)}{\\sum_i exp(x_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "<img src=\"./imgs/33.png\" >\n",
    "<img src=\"./imgs/34.png\" >\n",
    "<img src=\"./imgs/35.png\" >\n",
    "\n",
    "```python\n",
    "class torch.nn.Dropout(\n",
    "class torch.nn.Dropout2d(\n",
    "    p=0.5,         # % фичей, которые нужно занулить\n",
    "    inplace=False  # делать ли оптимизацию\n",
    "```\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "\n",
    "<img src=\"./imgs/36.png\" >\n",
    "\n",
    "Можно показать, что это происходит не только с входными данными, но и с выходами слоёв, даже если входные данные нормализованы (см [Andrej Karpathy cs231n](https://youtu.be/gYpoJMlgyXA?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&t=2221)).\n",
    "\n",
    "Решение проблемы - нормализация данных внутри сети - Batch Normalization [Ioffe, Szegedy 2015](https://arxiv.org/pdf/1502.03167.pdf)\n",
    "\n",
    "<img src=\"./imgs/37.png\" width=400>\n",
    "\n",
    "$$\n",
    "y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
    "$$\n",
    "\n",
    "<img src=\"./imgs/38.png\" width=400>\n",
    "\n",
    "\n",
    "```python\n",
    "class torch.nn.BatchNorm2d(\n",
    "    num_features,   # Число фильтров, которое следует ожидать на входе\n",
    "    eps=1e-05,      # Epsilon из знаменателя\n",
    "    momentum=0.1,   # Момент, с которым идет накопление среднего и дисперсии\n",
    "    affine=True)    # учить gamma и beta\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss функции\n",
    "\n",
    "```python\n",
    "class torch.nn.MSELoss\n",
    "class torch.nn.CrossEntropyLoss # Комбинация Cross Entropy и Softmax в одном слое\n",
    "class torch.nn.NLLLoss # Оптимизация правдоподобия, на вход ожидает логарифмы вероятностей (выход LogSoftmax)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решаем MNIST\n",
    "Решаем MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Загружаем MNIST\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original', data_home='./')\n",
    "X = mnist['data']\n",
    "# переходим к формату тензоров pytorch\n",
    "X = X.reshape((-1, 1, 28, 28))\n",
    "X = X.astype('float')\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD8CAYAAAA2Y2wxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQRAEy8hAKwiCYBkZaAVBECzz/84yIB7MXQpLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f90e4d83080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=4\n",
    "for i in range(n*n):\n",
    "    plt.subplot(n,n,i+1)\n",
    "    I = X[np.random.randint(0, X.shape[0]),0]\n",
    "    plt.imshow(I, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original [0 0 0 ..., 9 9 9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "Y = mnist['target'].astype('int')\n",
    "print('original', Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (fc1): Linear (256 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    # Слои, в которых нет параметров для обучения можно не создавать, а брать из переменной F\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # batchnorm\n",
    "        # conv in 28x28x1, out 24x24x8\n",
    "        # bn\n",
    "        # relu\n",
    "        # max pooling  # in 24x24, out 12x12\n",
    "        # conv in 12x12x8, out 8x8x16\n",
    "        # bn\n",
    "        # relu\n",
    "        # max pooling # in 8x8 out 4x4\n",
    "        # flatten\n",
    "        # linear\n",
    "        # relu\n",
    "        # log softmax\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        return x\n",
    "   \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "\n",
    "learning_rate = 1e-4\n",
    "batch_size = 10\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "loss_curve = []\n",
    "\n",
    "for iter in range(3):\n",
    "    Xperm = np.random.permutation(X.shape[0])\n",
    "    loss_acc = 0\n",
    "    for b in range(X.shape[0]//batch_size):\n",
    "        batch_idxs = Xperm[b*batch_size:(b+1)*batch_size]\n",
    "        x = Variable(torch.Tensor(X[batch_idxs]))\n",
    "        y = Variable(torch.LongTensor(Y[batch_idxs]))\n",
    "        \n",
    "        # TODO Optimization\n",
    "        \n",
    "    print('Done epoch %s with loss %s' % (iter, loss_acc))\n",
    "    loss_curve.append(loss_acc)\n",
    "    \n",
    "plt.plot(loss_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Версия для GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "net = Net().cuda()\n",
    "\n",
    "loss_fn = torch.nn.NLLLoss().cuda()\n",
    "\n",
    "learning_rate = 1e-4\n",
    "batch_size = 100\n",
    "loss_curve = []\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(3):\n",
    "    Xperm = np.random.permutation(X.shape[0])\n",
    "    loss_acc = 0\n",
    "    for b in range(X.shape[0]//batch_size):\n",
    "        batch_idxs = Xperm[b*batch_size:(b+1)*batch_size]\n",
    "        x = Variable(torch.Tensor(X[batch_idxs])).cuda()\n",
    "        y = Variable(torch.LongTensor(Y[batch_idxs])).cuda()\n",
    "        y_pred = net(x)\n",
    "              \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss_acc += loss.data[0]\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Done epoch %s with loss %s' % (iter, loss_acc))\n",
    "    loss_curve.append(loss_acc)\n",
    "    \n",
    "plt.plot(loss_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation\n",
    "\n",
    "<img src=\"./imgs/32.png\" >\n",
    "<img src=\"./imgs/41.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Work 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10\n",
    "<img src=\"./imgs/40.png\" width=600 >\n",
    "\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "- 32х32 цветные картинки\n",
    "- 10 не пересекающихся классов\n",
    "- 50к train + 10k test (сбалансированно по классам)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle\n",
    "\n",
    "https://www.kaggle.com/c/2017h2-mifi-bnn-hw1/host\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Почитать\n",
    "\n",
    "- cs231n\n",
    "  - https://www.youtube.com/watch?v=vT1JzLTH4G4&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk\n",
    "  - Лекция 5, 6, 7\n",
    "- http://pytorch.org/tutorials/index.html\n",
    "- https://m2dsupsdlclass.github.io/lectures-labs/slides/03_conv_nets/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/50.jpg\" width=600 >\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
